{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLProject_Bonus121.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJRTNUUBwkTX",
        "colab_type": "text"
      },
      "source": [
        "# Introduction \n",
        "This program is a neuronal newtork which can increase (7 by 7) images to (28 by 28) while keeping the global shape described by the starting image.\n",
        "\n",
        "*   The model uses the FashionMNIST database.\n",
        "*   We also tried to increase the quality of a (28 by 28) image by getting a model prediction with a (28 by 28) image during the model evalutions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSYkrCBiUF_x",
        "colab_type": "text"
      },
      "source": [
        "First we have to import all the libraries that we need\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJOa086STP-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from skimage.measure import block_reduce\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI0WEkri2wUe",
        "colab_type": "text"
      },
      "source": [
        "This cell allows to dowload the FashionMNIST dataset. \n",
        "We apply a modification on the content by the transform \"mnist_transform\" which is a transformation of all images to tensors. That is these tensors that we will use to train our model.\n",
        "\n",
        "Then we create two dataloader :\n",
        "\n",
        "*   The first one, train_loader is made from the train dataset of FashionMNIST.\n",
        "*   The second one, test_loader is made from the test dataset of FashionMNIST.\n",
        "\n",
        "Our train_loader and test_loader are composed by batchs of 32 images .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEttsSpoTRUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_transform = transforms.Compose([\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "#load train dataset\n",
        "train_dataset = datasets.FashionMNIST(root = '../data', train = True, download = True, transform = mnist_transform)\n",
        "#load test dataset\n",
        "test_dataset = datasets.FashionMNIST(root = '../data', train = False, download = True, transform = mnist_transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzIhmWxnGRiE",
        "colab_type": "text"
      },
      "source": [
        "The following cell creates a dataset composed by minimized images (From 28x28 to 7x7) by using an average pooling operation on our train and test datasets (based on the FashionMNIST dataset) to keep the global shape of the pattern on each image. \n",
        "\n",
        "Then we initialise two loader based on our new datasets, new_train_dataset and new_test_dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQC3sCSbTqcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_minimized_dataset(dataset):\n",
        "  new_dataset = []\n",
        "  for i_data in range(len(dataset)):\n",
        "    new_dataset.append([])\n",
        "    new_dataset[i_data].append(torch.tensor([block_reduce(dataset[i_data][0][0], block_size = (4,4), func=np.mean)])) #reducing by a scale of 4\n",
        "    new_dataset[i_data].append(dataset[i_data][0])\n",
        "  return new_dataset\n",
        "\n",
        "new_train_dataset = get_minimized_dataset(train_dataset)\n",
        "new_test_dataset = get_minimized_dataset(test_dataset)\n",
        "\n",
        "new_train_loader = DataLoader(new_train_dataset, batch_size = 32, shuffle = True)\n",
        "new_test_loader = DataLoader(new_test_dataset, batch_size = 32, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqaXM1R28vnv",
        "colab_type": "text"
      },
      "source": [
        "This class defines a convolutional block which will be implemented in our model.\n",
        "By default this block does not changes the input size and a relu activation function is applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P4D2kRkT0K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, act = F.relu):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels  = in_channels, #number of incoming chanels (3 for an image)\n",
        "            out_channels = out_channels, #number of outputing chanels\n",
        "            kernel_size  = kernel_size, #kernel size\n",
        "            stride       = stride,  #how many pixels the kernel is moving\n",
        "            padding      = padding, #number of pixel out of the frame\n",
        "        )\n",
        "        self.act = act\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWBhspze-u1p",
        "colab_type": "text"
      },
      "source": [
        "This class defines a convolutional upsample which will be usefull to increase by 2 the size of our images in our neuronal network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvcHRAJKT3qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvUpsample(nn.Module):\n",
        "    def __init__(self, kernel_size = 3, stride = 1, padding = 0):\n",
        "        super(ConvUpsample, self).__init__()   \n",
        "        self.conv = nn.Upsample(scale_factor = 2, mode='bilinear', align_corners=False) #create a bigger tensor with a scaling of 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4LCjHeIBbbr",
        "colab_type": "text"
      },
      "source": [
        "The goal of our model is to increase the resolution of a given image while increase the quality of the pattern described by the image.\n",
        "\n",
        "This model is composed by several convolution layer and performs two increase operation (from 7 * 7 to 14 * 14 and from 14 * 14 to 28 * 28).\n",
        "\n",
        "It uses some dropout to avoid dataset memorizing from model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pII__pR1T7Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SuperResolutionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SuperResolutionModel, self).__init__()\n",
        "\n",
        "        # We have tried many model configurations and this one is the most efficient\n",
        "        self.seq = nn.Sequential(\n",
        "            ConvUpsample(),\n",
        "            ConvBlock(1, 256),\n",
        "            ConvBlock(256, 512),\n",
        "            ConvBlock(512, 512),\n",
        "            nn.Dropout(p=0.2),\n",
        "            ConvBlock(512, 256),\n",
        "            ConvBlock(256, 128),\n",
        "            ConvBlock(128, 64),\n",
        "            ConvUpsample(),\n",
        "            ConvBlock(64, 128),\n",
        "            ConvBlock(128, 128),\n",
        "            nn.Dropout(p=0.1),\n",
        "            ConvBlock(128, 64),\n",
        "            ConvBlock(64, 32),\n",
        "            ConvBlock(32, 32),\n",
        "            ConvBlock(32, 1, act = nn.Sigmoid()) #Sets the activation function to a sigmoid, all values will be between 0 and 1 so they could be converted to pixel values. \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL796TCN8Rae",
        "colab_type": "text"
      },
      "source": [
        "This method allows to train our model. It calculates a prediction, and adjust the weights according to the result.\n",
        "\n",
        "At each epoch we call the evaluation function to know the model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmKI8KNZUHtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, criterion, optimizer, epochs, trainloader, testloader, device, scheduler): \n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, epochs))\n",
        "        print('-' * 10)\n",
        "        eval_loss  = evaluate(model, criterion, device, testloader, 20)\n",
        "        print(f' Eval : loss {eval_loss:6.7f} 'f'\\n')\n",
        "        for batch_id, (X, y) in enumerate(trainloader): #X = data given to the model for predictions , y = the expected result\n",
        "            X, y = X.to(device), y.to(device) #64 images 28x28\n",
        "            optimizer.zero_grad() #cleans the optimizer\n",
        "            y_pred = model(X) #gets the model prediction\n",
        "            loss   = criterion(y_pred, y) #calculates the prediction loss\n",
        "            loss.backward() #updates the weights to make future predictions closer to what we want \n",
        "            optimizer.step() #notifies the optimizer that we did a step \n",
        "        scheduler.step() #decreases the learning rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLIkve_Q3fVs",
        "colab_type": "text"
      },
      "source": [
        "This method goal is to evaluate the model accuracy. For each batch in the loader,\n",
        "it calculates the model prediction on an element and use the criterion for calculate the gap between the prediction and the desired result. This result is added to the loss.\n",
        "When n_batch have been calculated, the method shows 4 images :\n",
        "\n",
        "*   The first one is the image that has been given to the model.\n",
        "*   The second one is the desired image.\n",
        "*   The third one is the image made by the model.\n",
        "*   The last one is an image made by the model from the second image. This image is an attempt to obtain a high definition image by using our model with an already big image at start.\n",
        "\n",
        "\n",
        "Finally the function returns the average loss of all predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKFlTYyoUbib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, criterion, device, loader, n_batch = -1):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    total_pred = 0\n",
        "    figure = plt.figure()\n",
        "    with torch.no_grad():\n",
        "      for batch_id, (X, y) in enumerate(loader):\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            if (batch_id == n_batch):\n",
        "              figure.add_subplot(1, 3, 1)\n",
        "              plt.imshow(X[0][0].cpu().detach(), cmap = 'gray')\n",
        "              figure.add_subplot(1, 3, 2)\n",
        "              plt.imshow(y[0][0].cpu().detach(), cmap = 'gray')\n",
        "              figure.add_subplot(1, 3, 3)\n",
        "              plt.imshow(model(X)[0][0].cpu().detach(), cmap = 'gray')\n",
        "              plt.show()\n",
        "              plt.imshow(model(y)[0][0].cpu().detach(), cmap = 'gray')\n",
        "              plt.show()\n",
        "              return losses / total_pred\n",
        "            y_pred = model(X)\n",
        "            losses += criterion(y_pred, y)\n",
        "            total_pred  += len(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0sR2Of4Ucuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    epochs        = 4 #number of iterations on the database\n",
        "    learning_rate = 1e-3 #sets the starting learning rate\n",
        "    device = torch.device('cuda') #sets the device to the GPU\n",
        "    model = SuperResolutionModel() #creates the neuronal network\n",
        "    model = model.to(device) #sets the model in the device\n",
        "    optimizer     = optim.Adagrad( params = model.parameters(), lr = learning_rate )#setting the optimizer algorithm with wich update neuronal weights\n",
        "    scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.3)#the scheduler is used to update the learning rate gradually\n",
        "    criterion     = nn.MSELoss() #sets the loss function\n",
        "    train(model, criterion, optimizer, epochs, new_train_loader, new_test_loader, device, scheduler)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Noos8CIrUg_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}